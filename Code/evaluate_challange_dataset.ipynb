{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1575fa6-032e-46f9-8752-ffaf426cbf48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from data_utils import *\n",
    "from feature_utils import *\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "# Suppress FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7397fd9-0dc2-4359-8aff-bc5caf50d535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASELINE = False\n",
    "\n",
    "if BASELINE:\n",
    "    MODEL_NAME = '../models/distilbert-base-uncased-finetuned-baseline-argument-classification'\n",
    "else:\n",
    "    MODEL_NAME = '../models/distilbert-base-uncased-finetuned-advanced-argument-classification'\n",
    "    \n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c093cd6-9c38-43e2-bb27-7b574b5dd21c",
   "metadata": {},
   "source": [
    "Argument labels mapping to IDs differ from baseline and advanced models, because the order of distinct arguments in training\n",
    "changed when running for different models (unforseen error). However, during training the evaluation results on test set where saved at 'train resulst/' folder. Thus, the correct mappings were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ad37817-4385-4353-a1f4-1ebeb12ed856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if BASELINE:\n",
    "    arg_labels =[\"ARGM-ADJ\",\"ARGM-ADV\",\"ARGM-COM\",\"ARGM-MNR\",\n",
    "                 \"ARGM-PRR\",\"ARGM-PRP\",\"ARGM-REC\",\"O\",\n",
    "                 \"ARGM-CXN\",\"ARGM-DIR\",\"ARGM-DIS\",\"ARGM-CAU\",\n",
    "                 \"ARG3\",\"ARGA\",\"ARGM-LVB\",\"ARG5\",\n",
    "                 \"ARG2\",\"ARGM-MOD\",\"ARGM-LOC\",\"ARGM-NEG\",\n",
    "                 \"ARG1\",\"ARGM-EXT\",\"ARG0\",\"ARG4\",\n",
    "                  \"ARGM-PRD\",\"ARG1-DSP\",\"ARGM-GOL\",\"ARGM-TMP\"]\n",
    "    label2id = {label: i for i, label in enumerate(arg_labels)}\n",
    "    id2label = {i: label for i, label in enumerate(arg_labels)}\n",
    "else:\n",
    "    arg_labels =['ARGM-ADJ','ARGM-ADV','ARGM-COM','ARGM-MNR',\n",
    "                 'ARGM-PRR','ARGM-PRP','ARGM-REC','O','ARGM-CXN',\n",
    "                 'ARGM-DIR','ARGM-DIS','ARGM-CAU','ARG3','ARGA', \n",
    "                 'ARGM-LVB','ARG5','ARG2','ARGM-MOD','ARGM-LOC', \n",
    "                 'ARGM-NEG','ARG1','ARGM-EXT','ARG0','ARG4','ARGM-PRD',\n",
    "                 'ARG1-DSP','ARGM-GOL','ARGM-TMP']\n",
    "    label2id = {label: i for i, label in enumerate(arg_labels)}\n",
    "    id2label = {i: label for i, label in enumerate(arg_labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b62dc88-93cf-4c9c-beb5-038387b16c40",
   "metadata": {},
   "source": [
    "## functions for analyzing MFT test type:\n",
    "\n",
    "1) preprocess_mft_data - converts string like lists to python lists, adds predicate tokens to sentences depending on the model.\n",
    "2) tokenize_and_align_labels - Tokenizes the input examples and aligns argument labels and ids which is required due to   subtokenization from distilbert model.\n",
    "3) get_prediction - Gets predictions from tokenized data by aggregating subtoken logits so that it matches input lenghts.\n",
    "4) evaluate_predictions - calculate failure rate (failure_rate = count_of_fail_predictions / total_ARG0-4_occurances) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dc831b4-fd4d-4ca7-8c0d-0dcc97129ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_mft_data(data, baseline):\n",
    "    \"\"\"\n",
    "    Prepares mft dataset for predictions.\n",
    "    \n",
    "    params:\n",
    "    data: DataFrame of mft challange dataset.\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    # string split for sentence\n",
    "    data['sentence'] = data['sentence'].apply(lambda x: x.split())\n",
    "    # convert labels and is_predicate to string format\n",
    "    data['labels'] = data['labels'].apply(ast.literal_eval)\n",
    "    data['is_predicate'] = data['is_predicate'].apply(ast.literal_eval)\n",
    "\n",
    "    predicate_pos = [i.index('1') for i in data['is_predicate'].tolist()]\n",
    "    # add predicate tokens for baseline or advanced models\n",
    "    for idx, sent in enumerate(data['sentence']):\n",
    "        if baseline:\n",
    "            sent.append('[SEP]')\n",
    "            sent.append(sent[predicate_pos[idx]])\n",
    "            data['sentence'][idx] = sent\n",
    "            \n",
    "            data['labels'][idx].append('O')\n",
    "            data['labels'][idx].append('O')\n",
    "        else:\n",
    "            sent.insert(predicate_pos[idx], '[PREDICATE]')\n",
    "            data['sentence'][idx] = sent\n",
    "            data['labels'][idx].insert(predicate_pos[idx], 'O')\n",
    "\n",
    "    # map labels to integers corresponding to those in training\n",
    "    data['labels_mapped'] = data['labels'].apply(lambda x: [label2id[label] for label in x])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3eef9481-0cbf-46ee-9f4b-d6e9b3fec2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(data, sentence_ref, mapping_ref):\n",
    "    \"\"\"\n",
    "    Tokenizes the input examples and aligns argument labels and ids.\n",
    "\n",
    "    Parameters:\n",
    "    data: DataFrame containing tokens, sentence IDs, and argument labels/ids.\n",
    "    sentence_ref: sentences containing column name\n",
    "    mapping_ref: label_mapping containing column name\n",
    "\n",
    "    Returns:\n",
    "    list: A list of new examples with tokenized inputs and aligned labels.\n",
    "    \"\"\"\n",
    "    sentence_lists = data[sentence_ref].tolist()\n",
    "    sentence_ids = data['ID'].tolist()\n",
    "\n",
    "    # Tokenize sentences:\n",
    "    tokenized_inputs = tokenizer(sentence_lists, truncation=True, is_split_into_words=True)\n",
    "\n",
    "    aligned_examples = []\n",
    "    \n",
    "    for i,  arg_label in enumerate(data[mapping_ref]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        arg_ids = []\n",
    "        labels = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None: # set arg id and label to -100 for first and last special tokens\n",
    "                arg_ids.append(-100)\n",
    "                labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(arg_label[word_idx])\n",
    "            else:\n",
    "                labels.append(arg_label[word_idx])\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        \n",
    "        aligned_examples.append({\n",
    "            'sentence_id': sentence_ids[i],\n",
    "            'sentence': sentence_lists[i],\n",
    "            'word_ids': word_ids,\n",
    "            'input_ids': tokenized_inputs['input_ids'][i],\n",
    "            'attention_mask': tokenized_inputs['attention_mask'][i],\n",
    "            'labels': labels,\n",
    "        })\n",
    "        \n",
    "\n",
    "    return aligned_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29481361-c288-43e6-9b1d-fb7cad0fa4b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prediction(tokenized_examples):\n",
    "    '''\n",
    "    Gets predictions from tokenized data by aggregating subtoken logits so that\n",
    "    it matches input lenghts.\n",
    "    \n",
    "    params:\n",
    "    tokenized_examples: dict containing tokenizer outpus and labels\n",
    "    \n",
    "    returns:\n",
    "    predicted_labels: list of predicted labels mapped back to propbank annotations.\n",
    "    '''\n",
    "    predicted_labels = []\n",
    "    for example in tokenized_examples:\n",
    "        \n",
    "        input_ids = torch.tensor(example['input_ids']).unsqueeze(0)\n",
    "        attention_mask = torch.tensor(example['attention_mask']).unsqueeze(0)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        # subtoken aggregation is done by summing logits of given word subtokens and taking argmax.\n",
    "        aggregated_logits = aggregate_subtoken_logits([example], logits.detach().numpy())[0]\n",
    "        aggregated_predictions = np.argmax(aggregated_logits, axis=1)\n",
    "        # mapping arg ids back to propbank labels\n",
    "        pred_labels = [id2label[label_id] for label_id in aggregated_predictions]\n",
    "        predicted_labels.append(pred_labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b452e185-58e3-4158-96e3-6287a66cf692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(predicted_labels, gold_labels):\n",
    "    \"\"\"\n",
    "    Compute failure rate by counting incorrect predictions for ARG1-5 labels (ARGMs are not scope of challange data).\n",
    "    failure_rate = count_of_fail_predictions / total_ARG0-4_occurance\n",
    "    \n",
    "    params:\n",
    "    predicted_labels: list of predicted labels.\n",
    "    gold_labels: list of gold labels.\n",
    "    \n",
    "    returns: \n",
    "    failure_rate: float value.\n",
    "    \"\"\"\n",
    "    total_gold_labels = 0\n",
    "    fail_prediction = 0\n",
    "    for i in range(len(predicted_labels)):\n",
    "        \n",
    "        for pred, label in zip(predicted_labels[i], gold_labels[i]):\n",
    "            if label in ['ARG0', 'ARG1', 'ARG2', 'ARG3', 'ARG4']:\n",
    "                total_gold_labels += 1\n",
    "                if label != pred:\n",
    "                    fail_prediction += 1\n",
    "    failure_rate = fail_prediction/total_gold_labels\n",
    "    return total_gold_labels, fail_prediction,failure_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b63791-0337-4c44-ab2c-ff613f86d53c",
   "metadata": {},
   "source": [
    "# Conducting tests for MFT dataset.\n",
    "tested capabilities with minimal functionality test type dataset:\n",
    "1) Polysemy\n",
    "2) Passive voice comprehension\n",
    "3) Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58ff203c-e3af-42f3-b1bb-446d8015a2ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "      <th>capability</th>\n",
       "      <th>test_type</th>\n",
       "      <th>broad_capability</th>\n",
       "      <th>is_predicate</th>\n",
       "      <th>target_predicate</th>\n",
       "      <th>target_arguments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passive_voice_mrt_1</td>\n",
       "      <td>The dog was adopted by family .</td>\n",
       "      <td>['ARG1', 'ARG1', 'O', 'O', 'O', 'ARG0', 'O']</td>\n",
       "      <td>passive_voice</td>\n",
       "      <td>mft</td>\n",
       "      <td>argument_alternation</td>\n",
       "      <td>['0', '0', '0', '1', '0', '0', '0']</td>\n",
       "      <td>token 3</td>\n",
       "      <td>tokens 0, 1, 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>passive_voice_mrt_2</td>\n",
       "      <td>All the cookies have been eaten by children .</td>\n",
       "      <td>['ARG1', 'ARG1', 'ARG1', 'O', 'O', 'O', 'O', '...</td>\n",
       "      <td>passive_voice</td>\n",
       "      <td>mft</td>\n",
       "      <td>argument_alternation</td>\n",
       "      <td>['0', '0', '0', '0', '1', '0', '0', '0']</td>\n",
       "      <td>token4</td>\n",
       "      <td>tokens 0, 1, 6, 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>passive_voice_mrt_3</td>\n",
       "      <td>A novel was being written by the author .</td>\n",
       "      <td>['ARG1', 'ARG1', 'O', 'O', 'O', 'O', 'ARG0', '...</td>\n",
       "      <td>passive_voice</td>\n",
       "      <td>mft</td>\n",
       "      <td>argument_alternation</td>\n",
       "      <td>['0', '0', '0', '1', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>token 3</td>\n",
       "      <td>tokens 0, 1, 6, 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>passive_voice_mrt_4</td>\n",
       "      <td>The song was sung by the choir with great emot...</td>\n",
       "      <td>['ARG1', 'ARG1', 'O', 'O', 'O', 'ARG0', 'ARG0'...</td>\n",
       "      <td>passive_voice</td>\n",
       "      <td>mft</td>\n",
       "      <td>argument_alternation</td>\n",
       "      <td>['0', '0', '0', '1', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>token 3</td>\n",
       "      <td>tokens 0, 1, 5, 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>passive_voice_mrt_5</td>\n",
       "      <td>The project was completed by the group ahead o...</td>\n",
       "      <td>['ARG1', 'ARG1', 'O', 'O', 'O', 'ARG0', 'ARG0'...</td>\n",
       "      <td>passive_voice</td>\n",
       "      <td>mft</td>\n",
       "      <td>argument_alternation</td>\n",
       "      <td>['0', '0', '0', '1', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>token 3</td>\n",
       "      <td>tokens 0, 1, 5, 6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                                           sentence  \\\n",
       "0  passive_voice_mrt_1                    The dog was adopted by family .   \n",
       "1  passive_voice_mrt_2      All the cookies have been eaten by children .   \n",
       "2  passive_voice_mrt_3          A novel was being written by the author .   \n",
       "3  passive_voice_mrt_4  The song was sung by the choir with great emot...   \n",
       "4  passive_voice_mrt_5  The project was completed by the group ahead o...   \n",
       "\n",
       "                                              labels     capability test_type  \\\n",
       "0       ['ARG1', 'ARG1', 'O', 'O', 'O', 'ARG0', 'O']  passive_voice       mft   \n",
       "1  ['ARG1', 'ARG1', 'ARG1', 'O', 'O', 'O', 'O', '...  passive_voice       mft   \n",
       "2  ['ARG1', 'ARG1', 'O', 'O', 'O', 'O', 'ARG0', '...  passive_voice       mft   \n",
       "3  ['ARG1', 'ARG1', 'O', 'O', 'O', 'ARG0', 'ARG0'...  passive_voice       mft   \n",
       "4  ['ARG1', 'ARG1', 'O', 'O', 'O', 'ARG0', 'ARG0'...  passive_voice       mft   \n",
       "\n",
       "        broad_capability                                       is_predicate  \\\n",
       "0  argument_alternation                 ['0', '0', '0', '1', '0', '0', '0']   \n",
       "1  argument_alternation            ['0', '0', '0', '0', '1', '0', '0', '0']   \n",
       "2  argument_alternation   ['0', '0', '0', '1', '0', '0', '0', '0', '0', ...   \n",
       "3  argument_alternation   ['0', '0', '0', '1', '0', '0', '0', '0', '0', ...   \n",
       "4  argument_alternation   ['0', '0', '0', '1', '0', '0', '0', '0', '0', ...   \n",
       "\n",
       "  target_predicate   target_arguments  \n",
       "0          token 3     tokens 0, 1, 5  \n",
       "1           token4  tokens 0, 1, 6, 7  \n",
       "2          token 3  tokens 0, 1, 6, 7  \n",
       "3          token 3  tokens 0, 1, 5, 6  \n",
       "4          token 3  tokens 0, 1, 5, 6  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challange_mft_df = pd.read_csv('../Data/challangedataset_mft.csv', encoding='utf-8')\n",
    "challange_mft_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e988b4a-97b2-47ed-b897-347a2a4e5e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mft_df = process_mft_data(challange_mft_df, baseline=BASELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ab0b2d9-683e-4bb8-8b9a-afe2979f3028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize examples\n",
    "tokenized_examples_mft = tokenize_and_align_labels(mft_df,'sentence','labels_mapped')\n",
    "# get predictions\n",
    "predicted_labels = get_prediction(tokenized_examples_mft)\n",
    "# append prediction to mft_df\n",
    "mft_df['predicted_labels'] = predicted_labels\n",
    "# unique test names\n",
    "unique_tests = mft_df['capability'].unique()\n",
    "\n",
    "failure_rates = []\n",
    "target_totals = []\n",
    "fail_counts = []\n",
    "# looping over test names\n",
    "for capability in unique_tests:\n",
    "    # row indexes of distinct test\n",
    "    test_indexes = mft_df[mft_df['capability'] == capability].index\n",
    "    # get metrics from evaluation\n",
    "    total_gold_labels, total_fails, failure_rate = evaluate_predictions(predicted_labels[test_indexes[0]:test_indexes[-1]],\n",
    "                                        mft_df['labels'].tolist()[test_indexes[0]:test_indexes[-1]])\n",
    "    failure_rates.append(failure_rate), \n",
    "    target_totals.append(total_gold_labels)\n",
    "    fail_counts.append(total_fails)\n",
    "# saving results to dict\n",
    "results = {'capabilities': unique_tests,\n",
    "          'failure_rate': failure_rates,\n",
    "           'total_targets': target_totals,\n",
    "           'fail_count': fail_counts,\n",
    "           'test_types': ['MFT','MFT','MFT']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9dcce98-a991-4a70-8e4f-b56ec51e3c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capabilities</th>\n",
       "      <th>failure_rate</th>\n",
       "      <th>total_targets</th>\n",
       "      <th>fail_count</th>\n",
       "      <th>test_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passive_voice</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>MFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>polysemy</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>MFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robustness</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>MFT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    capabilities  failure_rate  total_targets  fail_count test_types\n",
       "0  passive_voice      0.611111             36          22        MFT\n",
       "1       polysemy      0.717949             39          28        MFT\n",
       "2     robustness      0.400000             15           6        MFT"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e12383d0-c9a9-456b-9984-05ffbf228817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if BASELINE:\n",
    "    # saving challange_mft data with label predictions by baseline model\n",
    "    # NOTE the sentences here include predicate tokens seperated by \"[SEP] predicate_word\" at the end of sequence\n",
    "    mft_df.to_csv('../evaluation results/challange_mft_with_predictions_baseline_model.csv')\n",
    "    # saving results to this file path\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('../evaluation results/mft_test_results_baseline_model.csv')\n",
    "\n",
    "else:\n",
    "    # saving challange_mft data with label predictions by baseline model\n",
    "    # NOTE the predicate words in the sentences have special token [PREDICATE] appended before it.\n",
    "    mft_df.to_csv('../evaluation results/challange_mft_with_predictions_advanced_model.csv')\n",
    "    # saving results to this file path\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'../evaluation results/mft_test_results_advanced_model.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3cc88-137c-4003-832e-b7753615ee70",
   "metadata": {},
   "source": [
    "## functions for analyzing INV test type:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19f7ecc3-b0ba-4ba1-9ab2-1c55926ad376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_inv_data(data, baseline):\n",
    "    \"\"\"\n",
    "    Prepares challange_inv dataset for tokenization.\n",
    "    \n",
    "    params:\n",
    "    data: DataFrame of mft challange dataset.\n",
    "    baseline: bool specifying baseline or else advanced model.\n",
    "    \n",
    "    returns:\n",
    "    data: processed DataFrame\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    # string split for sentence\n",
    "    data['sentence1'] = data['sentence1'].apply(lambda x: x.split())\n",
    "    data['sentence2'] = data['sentence2'].apply(lambda x: x.split())\n",
    "    # convert labels and is_predicate to string format\n",
    "    data['labels1'] = data['labels1'].apply(ast.literal_eval)\n",
    "    data['labels2'] = data['labels2'].apply(ast.literal_eval)\n",
    "\n",
    "    data['is_predicate1'] = data['is_predicate1'].apply(ast.literal_eval)\n",
    "    data['is_predicate2'] = data['is_predicate2'].apply(ast.literal_eval)\n",
    "    \n",
    "    data['target_tokens2'] = data['target_tokens2'].apply(ast.literal_eval)\n",
    "    data['target_tokens2'] = data['target_tokens2'].apply(lambda x: [int(i) for i in x])\n",
    "    data['expected_prediction'] = data['expected_prediction'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "    predicate_pos1 = [i.index('1') for i in data['is_predicate1'].tolist()]\n",
    "    predicate_pos2 = [i.index('1') for i in data['is_predicate2'].tolist()]\n",
    "    # add predicate tokens for baseline or advanced models\n",
    "    for idx, (sent1, sent2) in enumerate(zip(data['sentence1'], data['sentence2'])):\n",
    "        if baseline:\n",
    "            sent1.append('[SEP]')\n",
    "            sent1.append(sent1[predicate_pos1[idx]])\n",
    "            data['sentence1'][idx] = sent1\n",
    "            \n",
    "            data['labels1'][idx].append('O')\n",
    "            data['labels1'][idx].append('O')\n",
    "                                         \n",
    "            sent2.append('[SEP]')\n",
    "            sent2.append(sent2[predicate_pos2[idx]])\n",
    "            data['sentence2'][idx] = sent2\n",
    "            \n",
    "            data['labels2'][idx].append('O')\n",
    "            data['labels2'][idx].append('O')\n",
    "        else:\n",
    "            sent1.insert(predicate_pos1[idx], '[PREDICATE]')\n",
    "            data['sentence1'][idx] = sent1\n",
    "            data['labels1'][idx].insert(predicate_pos1[idx], 'O')\n",
    "                                         \n",
    "            sent2.insert(predicate_pos2[idx], '[PREDICATE]')\n",
    "            data['sentence2'][idx] = sent2\n",
    "            data['labels2'][idx].insert(predicate_pos2[idx], 'O')\n",
    "\n",
    "    # map labels to integers corresponding to those in training\n",
    "    data['labels_mapped1'] = data['labels1'].apply(lambda x: [label2id[label] for label in x])\n",
    "    data['labels_mapped2'] = data['labels2'].apply(lambda x: [label2id[label] for label in x])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1dcba3e-c41b-436f-89a0-eca746c758ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_invariance(predictions, target_tokens, expected_predictions):\n",
    "    \"\"\"\n",
    "    Evaluate invariance examples by checking if predictions match expected labels\n",
    "    after introducing variance.\n",
    "    \n",
    "    params:\n",
    "    predictions: list containing sequence prediction labels.\n",
    "    target_tokens: array-like object containing target token indexes.\n",
    "    expected_prediction: array-like object containing expected prediction labels.\n",
    "\n",
    "    returns:\n",
    "    total_targets: Total number of targets.\n",
    "    count_fails: Number of fail predictions.\n",
    "    failure_rate: float percentage of failure rate. \n",
    "    \"\"\"\n",
    "    \n",
    "    total_targets = 0\n",
    "    count_fails = 0\n",
    "\n",
    "    for idx, pred in enumerate(predictions):\n",
    "        current_targets = target_tokens[idx]\n",
    "        current_gold = expected_predictions[idx]\n",
    "        total_targets+=len(current_gold)\n",
    "        for target_index, target_label in zip(current_targets, current_gold):\n",
    "            \n",
    "            if pred[target_index] != target_label:\n",
    "                count_fails+=1\n",
    "    failure_rate = count_fails/total_targets\n",
    "\n",
    "    return total_targets, count_fails, failure_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990b177-81af-4256-b667-aa81db8d5c34",
   "metadata": {},
   "source": [
    "# Conducting tests for INV dataset.\n",
    "tested capabilities with Invariance test type dataset:\n",
    "1) Verbal phrase ellipsis\n",
    "2) Noun phrase ellipsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66840e4f-a869-4ae6-8508-9aad26639304",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>labels1</th>\n",
       "      <th>labels2</th>\n",
       "      <th>capability</th>\n",
       "      <th>test_type</th>\n",
       "      <th>is_predicate1</th>\n",
       "      <th>is_predicate2</th>\n",
       "      <th>target_tokens2</th>\n",
       "      <th>expected_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>verb_ellipsis_1</td>\n",
       "      <td>John can play the guitar , and Mary can play t...</td>\n",
       "      <td>John can play the guitar , and Mary can too .</td>\n",
       "      <td>['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O' ,'O', '...</td>\n",
       "      <td>['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...</td>\n",
       "      <td>verb_ellipsis</td>\n",
       "      <td>inv</td>\n",
       "      <td>['0', '0', '0', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>['0', '0', '1', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>['7']</td>\n",
       "      <td>['ARG0']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>verb_ellipsis_2</td>\n",
       "      <td>They have been to Italy , and we have been to ...</td>\n",
       "      <td>They have been to Italy , and we have as well .</td>\n",
       "      <td>['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...</td>\n",
       "      <td>['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...</td>\n",
       "      <td>verb_ellipsis</td>\n",
       "      <td>inv</td>\n",
       "      <td>['0', '0', '0', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>['0', '0', '1', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>['7']</td>\n",
       "      <td>['ARG0']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>verb_ellipsis_3</td>\n",
       "      <td>We could see the movie tonight , or we could s...</td>\n",
       "      <td>We could see the movie tonight , or we could t...</td>\n",
       "      <td>['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...</td>\n",
       "      <td>['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...</td>\n",
       "      <td>verb_ellipsis</td>\n",
       "      <td>inv</td>\n",
       "      <td>['0', '0', '0', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>['0', '0', '1', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>['8']</td>\n",
       "      <td>['ARG0']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>verb_ellipsis_4</td>\n",
       "      <td>You should call your mom more often , and you ...</td>\n",
       "      <td>You should call your mom more often, and your ...</td>\n",
       "      <td>['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...</td>\n",
       "      <td>['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...</td>\n",
       "      <td>verb_ellipsis</td>\n",
       "      <td>inv</td>\n",
       "      <td>['0', '0', '0', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>['0', '0', '1', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>['9', '10']</td>\n",
       "      <td>['ARG1', 'ARG1']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>verb_ellipsis_5</td>\n",
       "      <td>I used to play the piano , and my brother used...</td>\n",
       "      <td>I used to play the piano , and my brother the ...</td>\n",
       "      <td>['ARG0', 'O', 'O', 'O', 'ARG1', 'ARG1', 'O', '...</td>\n",
       "      <td>['ARG0', 'O', 'O', 'O', 'ARG1', 'ARG1', 'O', '...</td>\n",
       "      <td>verb_ellipsis</td>\n",
       "      <td>inv</td>\n",
       "      <td>['0', '0', '0', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>['0', '0', '0', '1', '0', '0', '0', '0', '0', ...</td>\n",
       "      <td>['7', '8' , '9', '10']</td>\n",
       "      <td>['ARG0', 'ARG0', 'ARG1','ARG1']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                          sentence1  \\\n",
       "0  verb_ellipsis_1  John can play the guitar , and Mary can play t...   \n",
       "1  verb_ellipsis_2  They have been to Italy , and we have been to ...   \n",
       "2  verb_ellipsis_3  We could see the movie tonight , or we could s...   \n",
       "3  verb_ellipsis_4  You should call your mom more often , and you ...   \n",
       "4  verb_ellipsis_5  I used to play the piano , and my brother used...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0      John can play the guitar , and Mary can too .   \n",
       "1    They have been to Italy , and we have as well .   \n",
       "2  We could see the movie tonight , or we could t...   \n",
       "3  You should call your mom more often, and your ...   \n",
       "4  I used to play the piano , and my brother the ...   \n",
       "\n",
       "                                             labels1  \\\n",
       "0  ['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O' ,'O', '...   \n",
       "1  ['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...   \n",
       "2  ['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...   \n",
       "3  ['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...   \n",
       "4  ['ARG0', 'O', 'O', 'O', 'ARG1', 'ARG1', 'O', '...   \n",
       "\n",
       "                                             labels2     capability test_type  \\\n",
       "0  ['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...  verb_ellipsis       inv   \n",
       "1  ['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...  verb_ellipsis       inv   \n",
       "2  ['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...  verb_ellipsis       inv   \n",
       "3  ['ARG0', 'O', 'O', 'ARG1', 'ARG1', 'O', 'O', '...  verb_ellipsis       inv   \n",
       "4  ['ARG0', 'O', 'O', 'O', 'ARG1', 'ARG1', 'O', '...  verb_ellipsis       inv   \n",
       "\n",
       "                                       is_predicate1  \\\n",
       "0  ['0', '0', '0', '0', '0', '0', '0', '0', '0', ...   \n",
       "1  ['0', '0', '0', '0', '0', '0', '0', '0', '0', ...   \n",
       "2  ['0', '0', '0', '0', '0', '0', '0', '0', '0', ...   \n",
       "3  ['0', '0', '0', '0', '0', '0', '0', '0', '0', ...   \n",
       "4  ['0', '0', '0', '0', '0', '0', '0', '0', '0', ...   \n",
       "\n",
       "                                       is_predicate2          target_tokens2  \\\n",
       "0  ['0', '0', '1', '0', '0', '0', '0', '0', '0', ...                   ['7']   \n",
       "1  ['0', '0', '1', '0', '0', '0', '0', '0', '0', ...                   ['7']   \n",
       "2  ['0', '0', '1', '0', '0', '0', '0', '0', '0', ...                   ['8']   \n",
       "3  ['0', '0', '1', '0', '0', '0', '0', '0', '0', ...             ['9', '10']   \n",
       "4  ['0', '0', '0', '1', '0', '0', '0', '0', '0', ...  ['7', '8' , '9', '10']   \n",
       "\n",
       "               expected_prediction  \n",
       "0                         ['ARG0']  \n",
       "1                         ['ARG0']  \n",
       "2                         ['ARG0']  \n",
       "3                 ['ARG1', 'ARG1']  \n",
       "4  ['ARG0', 'ARG0', 'ARG1','ARG1']  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challange_inv_df = pd.read_csv('../Data/challangedataset_inv.csv', encoding='utf-8')\n",
    "challange_inv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "197d0615-6ef9-4fc7-abc1-67cb8d87cfc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>labels1</th>\n",
       "      <th>labels2</th>\n",
       "      <th>capability</th>\n",
       "      <th>test_type</th>\n",
       "      <th>is_predicate1</th>\n",
       "      <th>is_predicate2</th>\n",
       "      <th>target_tokens2</th>\n",
       "      <th>expected_prediction</th>\n",
       "      <th>labels_mapped1</th>\n",
       "      <th>labels_mapped2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>verb_ellipsis_1</td>\n",
       "      <td>[John, can, play, the, guitar, ,, and, Mary, c...</td>\n",
       "      <td>[John, can, [PREDICATE], play, the, guitar, ,,...</td>\n",
       "      <td>[ARG0, O, O, ARG1, ARG1, O, O, ARG0, O, O, O, ...</td>\n",
       "      <td>[ARG0, O, O, O, ARG1, ARG1, O, O, ARG0, O, O, O]</td>\n",
       "      <td>verb_ellipsis</td>\n",
       "      <td>inv</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[ARG0]</td>\n",
       "      <td>[22, 7, 7, 20, 20, 7, 7, 22, 7, 7, 7, 7, 7]</td>\n",
       "      <td>[22, 7, 7, 7, 20, 20, 7, 7, 22, 7, 7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>verb_ellipsis_2</td>\n",
       "      <td>[They, have, been, to, Italy, ,, and, we, have...</td>\n",
       "      <td>[They, have, [PREDICATE], been, to, Italy, ,, ...</td>\n",
       "      <td>[ARG0, O, O, ARG1, ARG1, O, O, ARG0, O, O, O, ...</td>\n",
       "      <td>[ARG0, O, O, O, ARG1, ARG1, O, O, ARG0, O, O, ...</td>\n",
       "      <td>verb_ellipsis</td>\n",
       "      <td>inv</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[ARG0]</td>\n",
       "      <td>[22, 7, 7, 20, 20, 7, 7, 22, 7, 7, 7, 20, 20, ...</td>\n",
       "      <td>[22, 7, 7, 7, 20, 20, 7, 7, 22, 7, 7, 7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>verb_ellipsis_3</td>\n",
       "      <td>[We, could, see, the, movie, tonight, ,, or, w...</td>\n",
       "      <td>[We, could, [PREDICATE], see, the, movie, toni...</td>\n",
       "      <td>[ARG0, O, O, ARG1, ARG1, O, O, O, ARG0, O, O, ...</td>\n",
       "      <td>[ARG0, O, O, O, ARG1, ARG1, O, O, O, ARG0, O, ...</td>\n",
       "      <td>verb_ellipsis</td>\n",
       "      <td>inv</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[ARG0]</td>\n",
       "      <td>[22, 7, 7, 20, 20, 7, 7, 7, 22, 7, 7, 7, 20, 2...</td>\n",
       "      <td>[22, 7, 7, 7, 20, 20, 7, 7, 7, 22, 7, 7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>verb_ellipsis_4</td>\n",
       "      <td>[You, should, call, your, mom, more, often, ,,...</td>\n",
       "      <td>[You, should, [PREDICATE], call, your, mom, mo...</td>\n",
       "      <td>[ARG0, O, O, ARG1, ARG1, O, O, O, O, ARG0, O, ...</td>\n",
       "      <td>[ARG0, O, O, O, ARG1, ARG1, O, O, O, O, ARG1, ...</td>\n",
       "      <td>verb_ellipsis</td>\n",
       "      <td>inv</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>[ARG1, ARG1]</td>\n",
       "      <td>[22, 7, 7, 20, 20, 7, 7, 7, 7, 22, 7, 7, 7, 20...</td>\n",
       "      <td>[22, 7, 7, 7, 20, 20, 7, 7, 7, 7, 20, 20, 7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>verb_ellipsis_5</td>\n",
       "      <td>[I, used, to, play, the, piano, ,, and, my, br...</td>\n",
       "      <td>[I, used, to, [PREDICATE], play, the, piano, ,...</td>\n",
       "      <td>[ARG0, O, O, O, ARG1, ARG1, O, O, ARG0, ARG0, ...</td>\n",
       "      <td>[ARG0, O, O, O, O, ARG1, ARG1, O, O, ARG0, ARG...</td>\n",
       "      <td>verb_ellipsis</td>\n",
       "      <td>inv</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[7, 8, 9, 10]</td>\n",
       "      <td>[ARG0, ARG0, ARG1, ARG1]</td>\n",
       "      <td>[22, 7, 7, 7, 20, 20, 7, 7, 22, 22, 7, 7, 7, 7...</td>\n",
       "      <td>[22, 7, 7, 7, 7, 20, 20, 7, 7, 22, 22, 20, 20, 7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                          sentence1  \\\n",
       "0  verb_ellipsis_1  [John, can, play, the, guitar, ,, and, Mary, c...   \n",
       "1  verb_ellipsis_2  [They, have, been, to, Italy, ,, and, we, have...   \n",
       "2  verb_ellipsis_3  [We, could, see, the, movie, tonight, ,, or, w...   \n",
       "3  verb_ellipsis_4  [You, should, call, your, mom, more, often, ,,...   \n",
       "4  verb_ellipsis_5  [I, used, to, play, the, piano, ,, and, my, br...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  [John, can, [PREDICATE], play, the, guitar, ,,...   \n",
       "1  [They, have, [PREDICATE], been, to, Italy, ,, ...   \n",
       "2  [We, could, [PREDICATE], see, the, movie, toni...   \n",
       "3  [You, should, [PREDICATE], call, your, mom, mo...   \n",
       "4  [I, used, to, [PREDICATE], play, the, piano, ,...   \n",
       "\n",
       "                                             labels1  \\\n",
       "0  [ARG0, O, O, ARG1, ARG1, O, O, ARG0, O, O, O, ...   \n",
       "1  [ARG0, O, O, ARG1, ARG1, O, O, ARG0, O, O, O, ...   \n",
       "2  [ARG0, O, O, ARG1, ARG1, O, O, O, ARG0, O, O, ...   \n",
       "3  [ARG0, O, O, ARG1, ARG1, O, O, O, O, ARG0, O, ...   \n",
       "4  [ARG0, O, O, O, ARG1, ARG1, O, O, ARG0, ARG0, ...   \n",
       "\n",
       "                                             labels2     capability test_type  \\\n",
       "0   [ARG0, O, O, O, ARG1, ARG1, O, O, ARG0, O, O, O]  verb_ellipsis       inv   \n",
       "1  [ARG0, O, O, O, ARG1, ARG1, O, O, ARG0, O, O, ...  verb_ellipsis       inv   \n",
       "2  [ARG0, O, O, O, ARG1, ARG1, O, O, O, ARG0, O, ...  verb_ellipsis       inv   \n",
       "3  [ARG0, O, O, O, ARG1, ARG1, O, O, O, O, ARG1, ...  verb_ellipsis       inv   \n",
       "4  [ARG0, O, O, O, O, ARG1, ARG1, O, O, ARG0, ARG...  verb_ellipsis       inv   \n",
       "\n",
       "                                      is_predicate1  \\\n",
       "0              [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]   \n",
       "\n",
       "                             is_predicate2 target_tokens2  \\\n",
       "0        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]            [7]   \n",
       "1     [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]            [7]   \n",
       "2     [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]            [8]   \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]        [9, 10]   \n",
       "4  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [7, 8, 9, 10]   \n",
       "\n",
       "        expected_prediction  \\\n",
       "0                    [ARG0]   \n",
       "1                    [ARG0]   \n",
       "2                    [ARG0]   \n",
       "3              [ARG1, ARG1]   \n",
       "4  [ARG0, ARG0, ARG1, ARG1]   \n",
       "\n",
       "                                      labels_mapped1  \\\n",
       "0        [22, 7, 7, 20, 20, 7, 7, 22, 7, 7, 7, 7, 7]   \n",
       "1  [22, 7, 7, 20, 20, 7, 7, 22, 7, 7, 7, 20, 20, ...   \n",
       "2  [22, 7, 7, 20, 20, 7, 7, 7, 22, 7, 7, 7, 20, 2...   \n",
       "3  [22, 7, 7, 20, 20, 7, 7, 7, 7, 22, 7, 7, 7, 20...   \n",
       "4  [22, 7, 7, 7, 20, 20, 7, 7, 22, 22, 7, 7, 7, 7...   \n",
       "\n",
       "                                      labels_mapped2  \n",
       "0           [22, 7, 7, 7, 20, 20, 7, 7, 22, 7, 7, 7]  \n",
       "1        [22, 7, 7, 7, 20, 20, 7, 7, 22, 7, 7, 7, 7]  \n",
       "2        [22, 7, 7, 7, 20, 20, 7, 7, 7, 22, 7, 7, 7]  \n",
       "3    [22, 7, 7, 7, 20, 20, 7, 7, 7, 7, 20, 20, 7, 7]  \n",
       "4  [22, 7, 7, 7, 7, 20, 20, 7, 7, 22, 22, 20, 20, 7]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_df = process_inv_data(challange_inv_df, baseline=BASELINE)\n",
    "inv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ccc7224f-c919-4122-a275-cccac168b2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized sentence2 column which contain ellipsis examples\n",
    "tokenized_examples2_inv = tokenize_and_align_labels(inv_df, 'sentence2', 'labels_mapped2')\n",
    "# get predictions for sentence2 data\n",
    "predicted_labels2 = get_prediction(tokenized_examples2_inv)\n",
    "# unique tests\n",
    "unique_tests = inv_df['capability'].unique()\n",
    "# target tokens and expected predictions\n",
    "target_tokens = inv_df['target_tokens2'].tolist()\n",
    "expected_predictions = inv_df['expected_prediction'].tolist()\n",
    "\n",
    "# placeholders\n",
    "failure_rates = []\n",
    "target_totals = []\n",
    "fail_counts = []\n",
    "\n",
    "results = {}\n",
    "# loop over test names to get results per distinct test\n",
    "for capability in unique_tests:\n",
    "    test_results = {}\n",
    "    test_indexes = inv_df[inv_df['capability'] == capability].index\n",
    "    # getting evaluation metrics\n",
    "    total_gold_labels, total_fails, failure_rate = evaluate_invariance(predicted_labels2[test_indexes[0]:test_indexes[-1]],\n",
    "                                                                 target_tokens[test_indexes[0]:test_indexes[-1]],\n",
    "                                                                 expected_predictions[test_indexes[0]:test_indexes[-1]])\n",
    "    \n",
    "    failure_rates.append(failure_rate), \n",
    "    target_totals.append(total_gold_labels)\n",
    "    fail_counts.append(total_fails)\n",
    "# save results\n",
    "results = {'capabilities': unique_tests,\n",
    "          'failure_rate': failure_rates,\n",
    "           'total_targets': target_totals,\n",
    "           'fail_count': fail_counts,\n",
    "           'test_types': ['INV', 'INV']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e04c817d-6adb-4e60-90ec-52fa25822be2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capabilities': array(['verb_ellipsis', 'noun_ellipsis'], dtype=object),\n",
       " 'failure_rate': [1.0, 0.5555555555555556],\n",
       " 'total_targets': [5, 9],\n",
       " 'fail_count': [5, 5],\n",
       " 'test_types': ['INV', 'INV']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9bef79b6-c631-4fce-ab40-a0ce2c423244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if BASELINE:\n",
    "    # saving challange_inv data with label predictions by baseline model\n",
    "    # NOTE the sentences here include predicate tokens seperated by \"[SEP] predicate_word\" at the end of sequence\n",
    "    inv_df.to_csv('../evaluation results/challange_inv_with_predictions_baseline_model.csv')\n",
    "    # saving results to this file path\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('../evaluation results/inv_test_results_baseline_model.csv')\n",
    "\n",
    "else:\n",
    "    # saving challange_inv data with label predictions by baseline model\n",
    "    # NOTE the predicate words in the sentences have special token [PREDICATE] appended before it.\n",
    "    inv_df.to_csv('../evaluation results/challange_inv_with_predictions_advanced_model.csv')\n",
    "    # saving results to this file path\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('../evaluation results/inv_test_results_advanced_model.csv')\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
